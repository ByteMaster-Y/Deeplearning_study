import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt

"""

# CNNì„ ì´ìš©í•œ MNIST ìˆ«ì ë¶„ë¥˜

ì´ í”„ë¡œì íŠ¸ëŠ” **CNN(Convolutional Neural Network)**ì„ ì‚¬ìš©í•˜ì—¬ MNIST ìˆ«ì ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì„ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤.

## 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬

```python
(x_train, t_train), (x_test, t_test) = keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0  # ì •ê·œí™”
x_train = x_train[..., np.newaxis]  # (60000, 28, 28, 1) í˜•íƒœë¡œ ë³€í™˜
x_test = x_test[..., np.newaxis]
```

### ğŸ“Œ í•µì‹¬ í¬ì¸íŠ¸
- **MNIST ë°ì´í„°ì…‹ ë¡œë“œ**: 28x28 í¬ê¸°ì˜ ì†ê¸€ì”¨ ìˆ«ì ì´ë¯¸ì§€.
- **ì •ê·œí™”**: í”½ì…€ ê°’ì„ 0~1 ë²”ìœ„ë¡œ ë³€í™˜í•˜ì—¬ í•™ìŠµ ì„±ëŠ¥ í–¥ìƒ.
- **ì°¨ì› ë³€í™˜**: CNN ëª¨ë¸ì´ 4D ì…ë ¥(batch, height, width, channel)ì„ ìš”êµ¬í•˜ë¯€ë¡œ `(28,28)`ì„ `(28,28,1)`ë¡œ ë³€í™˜.

## 2. CNN ëª¨ë¸ êµ¬ì¡°

```python
model = keras.Sequential([
    layers.Conv2D(30, (5, 5), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(100, activation='relu'),
    layers.Dense(10, activation='softmax')
])
```

### ğŸ“Œ í•µì‹¬ í¬ì¸íŠ¸
1. **Conv2D(30, (5,5), activation='relu')**
   - 30ê°œì˜ 5Ã—5 í•„í„° ì ìš©í•˜ì—¬ íŠ¹ì§• ì¶”ì¶œ.
   - ReLU í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš©.
2. **MaxPooling2D((2,2))**
   - 2Ã—2 í’€ë§ì„ ì ìš©í•˜ì—¬ íŠ¹ì§• ë§µ í¬ê¸° ì ˆë°˜ ê°ì†Œ.
3. **Flatten()**
   - CNN ì¶œë ¥ì„ 1ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜.
4. **Dense(100, activation='relu')**
   - 100ê°œì˜ ë‰´ëŸ°ì„ ê°€ì§„ ì™„ì „ì—°ê²°ì¸µ.
5. **Dense(10, activation='softmax')**
   - 10ê°œì˜ í´ë˜ìŠ¤(ìˆ«ì 0~9)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì¶œë ¥ì¸µ.

## 3. ëª¨ë¸ ì»´íŒŒì¼ ë° í•™ìŠµ

```python
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
```

```python
history = model.fit(x_train[:5000], t_train[:5000], epochs=20, batch_size=100,
                    validation_data=(x_test[:1000], t_test[:1000]))
```

### ğŸ“Œ í•µì‹¬ í¬ì¸íŠ¸
- **Adam ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©**: ì ì‘í˜• í•™ìŠµë¥  ì¡°ì •.
- **ì†ì‹¤ í•¨ìˆ˜**: `sparse_categorical_crossentropy` (ì •ë‹µì´ ì›-í•« ì¸ì½”ë”©ì´ ì•„ë‹˜).
- **ë°°ì¹˜ í¬ê¸°(batch_size=100)**: ë©”ëª¨ë¦¬ì™€ ì„±ëŠ¥ ê· í˜• ì¡°ì ˆ.
- **ê²€ì¦ ë°ì´í„° ì„¤ì •**: `validation_data=(x_test[:1000], t_test[:1000])`ë¡œ í•™ìŠµ ì¤‘ ì„±ëŠ¥ í™•ì¸.

## 4. í•™ìŠµ ê²°ê³¼ ì‹œê°í™”

```python
plt.plot(history.history['accuracy'], marker='o', label='train', markevery=2)
plt.plot(history.history['val_accuracy'], marker='s', label='test', markevery=2)
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()
```

### ğŸ“Œ í•µì‹¬ í¬ì¸íŠ¸
- í•™ìŠµ ì •í™•ë„(`accuracy`)ì™€ ê²€ì¦ ì •í™•ë„(`val_accuracy`)ë¥¼ ë¹„êµí•˜ì—¬ **ê³¼ì í•© ì—¬ë¶€ íŒë‹¨**.
- í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ì˜ ì°¨ì´ê°€ í¬ë©´ **ê³¼ì í•© ê°€ëŠ¥ì„±**ì´ ìˆìŒ.

## 5. ìµœì¢… í‰ê°€

```python
final_acc = model.evaluate(x_test[:1000], t_test[:1000], verbose=0)[1]
print(f"Final Test Accuracy: {final_acc:.3f}")
```

### ğŸ“Œ í•µì‹¬ í¬ì¸íŠ¸
- `evaluate()`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í™•ì¸.
- í…ŒìŠ¤íŠ¸ ì •í™•ë„ê°€ **95% ì´ìƒì´ë©´ ì„±ê³µì ì¸ ëª¨ë¸**.

---

## ğŸ”¥ ì§‘ì¤‘í•´ì„œ ë´ì•¼ í•  ë¶€ë¶„

### âœ… ë°ì´í„° ì „ì²˜ë¦¬
- `np.newaxis`ë¡œ CNN ì…ë ¥ í˜•íƒœ ë§ì¶”ê¸°.
- í”½ì…€ ê°’ ì •ê·œí™”(0~1).

### âœ… CNN êµ¬ì¡°
- `Conv2D` í•„í„° ê°œìˆ˜, í¬ê¸° ì¡°ì •ì´ ëª¨ë¸ ì„±ëŠ¥ì— ì˜í–¥.
- `MaxPooling2D`ë¥¼ ì‚¬ìš©í•´ ì—°ì‚°ëŸ‰ ê°ì†Œ.
- ë§ˆì§€ë§‰ `Dense(10, softmax)`ì—ì„œ í´ë˜ìŠ¤ í™•ë¥  ì˜ˆì¸¡.

### âœ… í•™ìŠµ ê³¼ì • ë° ê²°ê³¼ ë¶„ì„
- `batch_size`, `epochs` ì¡°ì ˆí•˜ì—¬ ìµœì í™”.
- í•™ìŠµ/ê²€ì¦ ì •í™•ë„ ë¹„êµë¡œ ê³¼ì í•© ì—¬ë¶€ í™•ì¸.

### âœ… ìµœì¢… ëª¨ë¸ í‰ê°€
- `evaluate()`ë¥¼ í†µí•´ ì¼ë°˜í™” ì„±ëŠ¥ í™•ì¸.
- ì •í™•ë„ê°€ ë‚®ë‹¤ë©´ **ëª¨ë¸ êµ¬ì¡° ì¡°ì •, ë°ì´í„° ì¦ê°•, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹** í•„ìš”.

---

ì´ì œ CNNì„ í™œìš©í•œ MNIST ë¶„ë¥˜ ëª¨ë¸ì„ ì´í•´í•˜ê³ , ì‹¤í—˜í•´ë³¼ ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! ğŸš€

"""


# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
(x_train, t_train), (x_test, t_test) = keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0  # ì •ê·œí™”
x_train = x_train[..., np.newaxis]  # (60000, 28, 28, 1) í˜•íƒœë¡œ ë³€í™˜
x_test = x_test[..., np.newaxis]

# ëª¨ë¸ ìƒì„±
model = keras.Sequential([
    layers.Conv2D(30, (5, 5), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(100, activation='relu'),
    layers.Dense(10, activation='softmax')
])

# ëª¨ë¸ ì»´íŒŒì¼
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# ëª¨ë¸ í•™ìŠµ
history = model.fit(x_train[:5000], t_train[:5000], epochs=20, batch_size=100,
                    validation_data=(x_test[:1000], t_test[:1000]))

# í•™ìŠµ ê²°ê³¼ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
plt.plot(history.history['accuracy'], marker='o', label='train', markevery=2)
plt.plot(history.history['val_accuracy'], marker='s', label='test', markevery=2)
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()

# ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„ ì¶œë ¥
final_acc = model.evaluate(x_test[:1000], t_test[:1000], verbose=0)[1]
print(f"Final Test Accuracy: {final_acc:.3f}")
