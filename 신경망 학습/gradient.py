import numpy as np
import matplotlib.pylab as plt

# ì´ ì½”ë“œëŠ” ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent) ì„ êµ¬í˜„í•œ ì½”ë“œì…ë‹ˆë‹¤.
# ê²½ì‚¬ í•˜ê°•ë²•ì€ ê¸°ìš¸ê¸°(Gradient)ë¥¼ ì´ìš©í•˜ì—¬ í•¨ìˆ˜ì˜ ìµœì†Ÿê°’ì„ ì°¾ëŠ” ìµœì í™” ë°©ë²•ì…ë‹ˆë‹¤.
# ë”¥ëŸ¬ë‹ì—ì„œ ì†ì‹¤ í•¨ìˆ˜(Loss Function)ë¥¼ ìµœì†Œí™”í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.

# ğŸ’¡ ê²½ì‚¬ í•˜ê°•ë²•ì„ ê°ìœ¼ë¡œ ì´í•´í•˜ì!

# ê¸°ìš¸ê¸°(Gradient)ëŠ” í˜„ì¬ ìœ„ì¹˜ì—ì„œ ì–´ë””ë¡œ ì´ë™í•´ì•¼ í• ì§€ ë°©í–¥ì„ ì•Œë ¤ì£¼ëŠ” ë‚˜ì¹¨ë°˜
# í•™ìŠµë¥ (Learning Rate)ì€ ì–¼ë§ˆë‚˜ ë¹ ë¥´ê²Œ ì´ë™í• ì§€ ì¡°ì ˆí•˜ëŠ” ì†ë„ ì¡°ì ˆê¸°
# ê²½ì‚¬ í•˜ê°•ë²•ì€ ê¸°ìš¸ê¸°ë¥¼ ë”°ë¼ ì ì  ë‚´ë ¤ê°€ë©´ì„œ ìµœì†Ÿê°’ì„ ì°¾ëŠ” ê³¼ì •
# ğŸ“Œ ì‹¤ì œë¡œ ë”¥ëŸ¬ë‹ì—ì„œëŠ” ìë™ ë¯¸ë¶„(Autograd)ì´ ê³„ì‚°ì„ í•´ì¤Œ
# â†’ ìš°ë¦¬ê°€ ì§ì ‘ ë¯¸ë¶„ì„ ê³„ì‚°í•  í•„ìš” ì—†ìŒ!
# â†’ PyTorch, TensorFlow ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ë‹¤ ì²˜ë¦¬í•´ ì¤Œ!

# ğŸ”¹ ê²°ë¡ : ìˆ˜ì‹ ìì²´ë¥¼ ì™¸ìš¸ í•„ìš”ëŠ” ì—†ì§€ë§Œ,
# ğŸ”¹ "ì™œ í•„ìš”í•œì§€" ì™€ "ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€" ëŠ” ì´í•´í•´ì•¼ í•¨! ğŸ˜Š

# ì• ì ˆì—ì„œ x0, x1ì— ëŒ€í•œ í¸ë¯¸ë¶„ì„ ë³€ìˆ˜ë³„ë¡œ ë”°ë¡œ ê³„ì‚°í–ˆìŒ.
# x0, x1ì˜ í¸ë¯¸ë¶„ì„ ë™ì‹œì— ê³„ì‚°í•˜ê³  ì‹¶ë‹¤ë©´?
# x0 = 3, x1 = 4ì¼ ë•Œ (x0, x1) ì–‘ìª½ì˜ í¸ë¯¸ë¶„ì„ ë¬¶ì–´ ë²¡í„°ë¡œ ì •ë¦¬í•œ ê²ƒì„ ê¸°ìš¸ê¸°gradientë¼ê³  í•œë‹¤.
def numerical_gradient(f, x):
    h = 1e-4
    grad = np.zeros_like(x)  # xì™€ í˜•ìƒì´ ê°™ì€ ë°°ì—´ì„ ìƒì„±

    for idx in range(x.size):
        tmp_val = x[idx]
        # f(x+h) ê³„ì‚°
        x[idx] = tmp_val + h
        fxh1 = f(x)

        # f(x-h) ê³„ì‚°
        x[idx] = tmp_val - h
        fxh2 = f(x)

        grad[idx] = (fxh1 - fxh2) / (2 * h)
        x[idx] = tmp_val  # ê°’ ë³µì›

    return grad


# f(x0, x1) = x0Â² + x1Â²
def function_2(x):
    return x[0]**2 + x[1]**2
    # or return np.sum(x**2)


print(numerical_gradient(function_2, np.array([3.0, 4.0])))  # [ 6.  8.]
print(numerical_gradient(function_2, np.array([0.0, 2.0])))  # [ 0.  4.]
print(numerical_gradient(function_2, np.array([3.0, 0.0])))  # [ 6.  0.]

# 4.4.1 ê²½ì‚¬ë²•(ê²½ì‚¬ í•˜ê°•ë²•)
# x0 = x0 - Î·*âˆ‚f/âˆ‚x0
# x1 = x1 - Î·*âˆ‚f/âˆ‚x1
# Î·(eta) : ê°±ì‹ í•˜ëŠ” ì–‘, í•™ìŠµë¥ learning rate
# ìœ„ ì‹ì„ ë°˜ë³µ


# f:ìµœì í™”í•˜ë ¤ëŠ” í•¨ìˆ˜
# init_x : ì´ˆê¹ƒê°’
# lr : í•™ìŠµë¥ 
# step_num : ë°˜ë³µíšŸìˆ˜
def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x
    x_history = []

    for i in range(step_num):
        x_history.append(x.copy())
        grad = numerical_gradient(f, x)
        x -= lr * grad

    return x, np.array(x_history)


# ê²½ì‚¬ë²•ìœ¼ë¡œ f(x0, x1) = x0Â² + x1Â²ì˜ ìµœì†Ÿê°’ì„ êµ¬í•´ë¼
init_x = np.array([-3.0, 4.0])
x, x_history = gradient_descent(function_2, init_x, lr=0.1)
print(x)  # [ -6.11110793e-10   8.14814391e-10]

# í•™ìŠµë¥ ì´ ë„ˆë¬´ í¼
init_x = np.array([-3.0, 4.0])
x, x_history = gradient_descent(function_2, init_x, lr=10.0)
print(x)  # [ -2.58983747e+13  -1.29524862e+12] ë°œì‚°í•¨

# í•™ìŠµë¥ ì´ ë„ˆë¬´ ì‘ìŒ
init_x = np.array([-3.0, 4.0])
x, x_history = gradient_descent(function_2, init_x, lr=1e-10)
print(x)  # [-2.99999994  3.99999992] ê±°ì˜ ë³€í™” ì—†ìŒ

# ê·¸ë˜í”„
init_x = np.array([-3.0, 4.0])
x, x_history = gradient_descent(function_2, init_x, lr=0.1, step_num=20)

plt.plot([-5, 5], [0, 0], '--b')
plt.plot([0, 0], [-5, 5], '--b')
plt.plot(x_history[:, 0], x_history[:, 1], 'o')

plt.xlim(-3.5, 3.5)
plt.ylim(-4.5, 4.5)
plt.xlabel("X0")
plt.ylabel("X1")
plt.show()