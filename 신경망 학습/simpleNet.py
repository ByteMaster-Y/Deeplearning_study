import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers


# ğŸ” ì½”ë“œ ìƒì„¸ ì„¤ëª…

# ğŸ“Œ 1. ì‹ ê²½ë§ ì •ì˜ (model)
# Dense(3, input_shape=(2,), activation=None)
# ì…ë ¥ í¬ê¸°: 2 (íŠ¹ì§• 2ê°œ)
# ì¶œë ¥ í¬ê¸°: 3 (í´ë˜ìŠ¤ 3ê°œ)
# activation=None: í™œì„±í™” í•¨ìˆ˜ ì—†ìŒ (ì†Œí”„íŠ¸ë§¥ìŠ¤ ì ìš© ì „ ë¡œì§“ ê°’ ë°˜í™˜)

# ğŸ“Œ 2. ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì • (compile())
# SGD (Stochastic Gradient Descent): í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•
# í•™ìŠµë¥  0.1ë¡œ ì„¤ì •
# gradient descentë¥¼ ì‚¬ìš©í•´ ê°€ì¤‘ì¹˜ë¥¼ ê°±ì‹ í•  ì˜ˆì •
# CategoricalCrossentropy (from_logits=True)
# from_logits=Trueì´ë¯€ë¡œ, Denseì˜ ì¶œë ¥ê°’(logits)ì— ì§ì ‘ ì ìš©
# ë‚´ë¶€ì ìœ¼ë¡œ softmax + cross entropyë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•¨

# ğŸ“Œ 3. ì…ë ¥ê°’ & ì •ë‹µ ë ˆì´ë¸” (x, t)
# x = np.array([[0.6, 0.9]])
# ì…ë ¥ ë°ì´í„° 2ê°œ (íŠ¹ì§•ì´ 2ê°œì¸ ìƒ˜í”Œ)
# t = np.array([[0, 0, 1]])
# ì •ë‹µì´ index 2ì¸ one-hot encoding ë ˆì´ë¸”

# ğŸ“Œ 4. ì˜ˆì¸¡ê°’ ê³„ì‚° (model.predict(x))
# model.predict(x)ëŠ” Dense ì¸µì˜ ì„ í˜• ë³€í™˜ ê²°ê³¼ ë¡œì§“(logits) ì„ ë°˜í™˜
# np.argmax(p): ê°€ì¥ í° ê°’ì„ ê°€ì§„ ì¸ë±ìŠ¤(ì¦‰, ì˜ˆì¸¡í•œ í´ë˜ìŠ¤)

# ğŸ“Œ 5. ì†ì‹¤ ê³„ì‚° (model.evaluate(x, t))
# í˜„ì¬ ê°€ì¤‘ì¹˜ ìƒíƒœì—ì„œ ì†ì‹¤ê°’(ì˜¤ì°¨) ë¥¼ ê³„ì‚°
# evaluate() í•¨ìˆ˜ëŠ” predict() + loss ê³„ì‚°ì„ í•œë²ˆì— ìˆ˜í–‰

# ğŸ“Œ 6. ê¸°ìš¸ê¸°(gradient) ê³„ì‚° (GradientTape)
# tf.GradientTape() ì‚¬ìš©
# GradientTapeëŠ” ì—°ì‚° ê³¼ì •ì„ ê¸°ë¡í•˜ì—¬ ìë™ ë¯¸ë¶„ì„ ê°€ëŠ¥í•˜ê²Œ í•¨
# ìˆœì „íŒŒ(logits = model(x, training=True)) ìˆ˜í–‰ í›„, ì†ì‹¤ê°’ ê³„ì‚°
# tape.gradient(loss_value, model.trainable_variables) í˜¸ì¶œ ì‹œ ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ê¸°ìš¸ê¸° ìë™ ê³„ì‚°

# ğŸ”¥ ê²°ë¡ : ì´ ì½”ë“œê°€ í•˜ëŠ” ì¼
# Dense(2 â†’ 3)ë¡œ ì´ë£¨ì–´ì§„ ì‹ ê²½ë§ ìƒì„±
# ì…ë ¥ê°’ x = [0.6, 0.9]ì„ ë°›ì•„ ì˜ˆì¸¡ê°’(logits) ê³„ì‚°
# ì •ë‹µ t = [0, 0, 1]ê³¼ ë¹„êµí•˜ì—¬ ì†ì‹¤ê°’ ì¸¡ì •
# GradientTapeë¡œ ì†ì‹¤ì„ ê°€ì¤‘ì¹˜ Wì— ëŒ€í•´ ë¯¸ë¶„ (ê¸°ìš¸ê¸° ê³„ì‚°)
# ì´í›„ SGDë¥¼ ì‚¬ìš©í•´ ê°€ì¤‘ì¹˜ ê°±ì‹  ê°€ëŠ¥ (í˜„ì¬ ì½”ë“œëŠ” í•™ìŠµ ê³¼ì • í¬í•¨ X)

# 1. ì‹ ê²½ë§ ëª¨ë¸ ì •ì˜
model = keras.Sequential([
    layers.Dense(3, input_shape=(2,), activation=None)  # ì…ë ¥ í¬ê¸° 2, ì¶œë ¥ í¬ê¸° 3
    # activation=None -> ì†Œí”„íŠ¸ë§¥ìŠ¤ ì ìš© ì „, ë¡œì§“(logits) ê°’ ì¶œë ¥
])

# 2. ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •
model.compile(
    optimizer=keras.optimizers.SGD(learning_rate=0.1),  # ê²½ì‚¬ í•˜ê°•ë²•(Stochastic Gradient Descent)
    loss=keras.losses.CategoricalCrossentropy(from_logits=True)  
    # ì†Œí”„íŠ¸ë§¥ìŠ¤ + í¬ë¡œìŠ¤ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜ (from_logits=True ì„¤ì •)
)

# 3. ì„ì˜ì˜ ì…ë ¥ê°’ (ì˜ˆì œ ë°ì´í„°)
x = np.array([[0.6, 0.9]])  # ì…ë ¥ ë°ì´í„° (2ê°œì˜ íŠ¹ì§•)
t = np.array([[0, 0, 1]])  # ì •ë‹µ ë ˆì´ë¸” (one-hot encoding, ì •ë‹µì€ index 2)

# 4. ì˜ˆì¸¡ê°’ ê³„ì‚°
p = model.predict(x)  # ì‹ ê²½ë§ì´ ì˜ˆì¸¡í•œ ë¡œì§“(logits) ê°’
print("ì˜ˆì¸¡ê°’ (logits):", p)
print("ìµœëŒ“ê°’ ì¸ë±ìŠ¤ (ì˜ˆì¸¡ í´ë˜ìŠ¤):", np.argmax(p))  # ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ ì¶œë ¥

# 5. ì†ì‹¤ ê³„ì‚° (í˜„ì¬ ê°€ì¤‘ì¹˜ ìƒíƒœì—ì„œ ì†ì‹¤ê°’ í™•ì¸)
loss = model.evaluate(x, t, verbose=0)  # ì†ì‹¤ ê°’ ê³„ì‚° (ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µ ë¹„êµ)
print("ì†ì‹¤ ê°’:", loss)

# with ë¬¸ì€ íŒŒì´ì¬ì—ì„œ íŠ¹ì • ë¸”ë¡ì˜ ì‹¤í–‰ì„ ê´€ë¦¬í•˜ëŠ” ê¸°ëŠ¥ì´ì•¼.
# íŠ¹íˆ TensorFlowì—ì„œ with tf.GradientTape()ëŠ” ìë™ìœ¼ë¡œ ë¯¸ë¶„ì„ ê³„ì‚°í•˜ëŠ” ì—­í• ì„ í•´!
# 6. ê¸°ìš¸ê¸° ê³„ì‚° (GradientTape ì‚¬ìš©)
with tf.GradientTape() as tape:
    logits = model(x, training=True)  # ìˆœì „íŒŒ(forward propagation) ìˆ˜í–‰ (logits ê³„ì‚°)
    loss_value = keras.losses.categorical_crossentropy(t, logits, from_logits=True)
    # ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚° (ì†Œí”„íŠ¸ë§¥ìŠ¤ ì ìš© ì—†ì´ ë¡œì§“ ê°’ì„ ë°”ë¡œ ì…ë ¥)

# 7. ê¸°ìš¸ê¸°(gradient) ê³„ì‚°: ì†ì‹¤ ê°’ì„ ê°€ì¤‘ì¹˜ Wì— ëŒ€í•´ ë¯¸ë¶„
grads = tape.gradient(loss_value, model.trainable_variables)
print("ê¸°ìš¸ê¸° (ê°€ì¤‘ì¹˜ Wì— ëŒ€í•œ ë¯¸ë¶„ê°’):", grads[0].numpy())  # ì²« ë²ˆì§¸ ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ê¸°ìš¸ê¸° ì¶œë ¥
