import sys
import os
import numpy as np
from collections import OrderedDict
sys.path.append(os.pardir)
from common.layers import *
from common.gradient import numerical_gradient

# 5.7.1 ì‹ ê²½ë§ í•™ìŠµì˜ ì „ì²´ ê·¸ë¦¼
"""
(4.5ì™€ ë™ì¼)
ì „ì œ
ì‹ ê²½ë§ì—ëŠ” ì ì‘ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì´ ìˆê³ , ì´ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ í›ˆë ¨ ë°ì´í„°ì— ì ì‘í•˜ë„ë¡ ì¡°ì •í•˜ëŠ” ê³¼ì •ì„ 'í•™ìŠµ'ì´ë¼ í•œë‹¤.
ì‹ ê²½ë§ í•™ìŠµì€ ë‹¤ìŒê³¼ ê°™ì´ 4ë‹¨ê³„ë¡œ ìˆ˜í–‰í•œë‹¤.

1ë‹¨ê³„ - ë¯¸ë‹ˆë°°ì¹˜
í›ˆë ¨ ë°ì´í„° ì¤‘ ì¼ë¶€ë¥¼ ë¬´ì‘ìœ„ë¡œ ê°€ì ¸ì˜¨ë‹¤. ì´ë ‡ê²Œ ì„ ë³„í•œ ë°ì´í„°ë¥¼ ë¯¸ë‹ˆë°°ì¹˜ë¼ í•˜ë©°,
ê·¸ ë¯¸ë‹ˆë°°ì¹˜ì˜ ì†ì‹¤í•¨ìˆ˜ ê°’ì„ ì¤„ì´ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤.

2ë‹¨ê³„ - ê¸°ìš¸ê¸° ì‚°ì¶œ
ë¯¸ë‹ˆë°°ì¹˜ì˜ ì†ì‹¤ í•¨ìˆ˜ ê°’ì„ ì¤„ì´ê¸° ìœ„í•´ ê° ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•œë‹¤.
ê¸°ìš¸ê¸°ëŠ” ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì„ ê°€ì¥ ì‘ê²Œ í•˜ëŠ” ë°©í–¥ì„ ì œì‹œí•œë‹¤.

3ë‹¨ê³„ - ë§¤ê°œë³€ìˆ˜ ê°±ì‹ 
ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê¸°ìš¸ê¸° ë°©í–¥ìœ¼ë¡œ ì•„ì£¼ ì¡°ê¸ˆ ê°±ì‹ í•œë‹¤.

4ë‹¨ê³„ - ë°˜ë³µ
1~3ë‹¨ê³„ë¥¼ ë°˜ë³µí•œë‹¤.

ìˆ˜ì¹˜ ë¯¸ë¶„ê³¼ ì˜¤ì°¨ì—­ì „íŒŒë²•ì€ 2ë‹¨ê³„ì—ì„œ ì‚¬ìš©
ìˆ˜ì¹˜ ë¯¸ë¶„ì€ êµ¬í˜„ì€ ì‰½ì§€ë§Œ ê³„ì‚°ì´ ì˜¤ë˜ê±¸ë¦¼
ì˜¤ì°¨ì—­ì „íŒŒë²•ì„ í†µí•´ ê¸°ìš¸ê¸°ë¥¼ íš¨ìœ¨ì ì´ê³  ë¹ ë¥´ê²Œ êµ¬í•  ìˆ˜ ìˆìŒ
"""

# 5.7.2 ì˜¤ì°¨ì—­ì „íŒŒë²•ì„ ì´ìš©í•œ ì‹ ê²½ë§ êµ¬í˜„í•˜ê¸°

"""
TwoLayerNet í´ë˜ìŠ¤ë¡œ êµ¬í˜„
 * í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜
params : ì‹ ê²½ë§ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ë³´ê´€í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ë³€ìˆ˜.
        params['W1']ì€ 1ë²ˆì§¸ ì¸µì˜ ê°€ì¤‘ì¹˜, params['b1']ì€ 1ë²ˆì§¸ ì¸µì˜ í¸í–¥.
        params['W2']ì€ 2ë²ˆì§¸ ì¸µì˜ ê°€ì¤‘ì¹˜, params['b2']ì€ 2ë²ˆì§¸ ì¸µì˜ í¸í–¥.
layers : ì‹ ê²½ë§ì˜ ê³„ì¸µì„ ë³´ê´€í•˜ëŠ” ìˆœì„œê°€ ìˆëŠ” ë”•ì…”ë„ˆë¦¬ ë³€ìˆ˜
        layers['Affine1'], layers['Relu1'], layers['Affine2']ì™€ ê°™ì´
        ê° ê³„ì¸µì„ ìˆœì„œëŒ€ë¡œ ìœ ì§€
lastLayer : ì‹ ê²½ë§ì˜ ë§ˆì§€ë§‰ ê³„ì¸µ(ì—¬ê¸°ì„œëŠ” SoftmaxWithLoss)

 * í´ë˜ìŠ¤ì˜ ë©”ì„œë“œ
__init__(...) : ì´ˆê¸°í™” ìˆ˜í–‰
predict(x) : ì˜ˆì¸¡(ì¶”ë¡ )ì„ ìˆ˜í–‰í•œë‹¤. xëŠ” ì´ë¯¸ì§€ ë°ì´í„°
loss(x, t) : ì†ì‹¤í•¨ìˆ˜ì˜ ê°’ì„ êµ¬í•œë‹¤. xëŠ” ì´ë¯¸ì§€ ë°ì´í„°, tëŠ” ì •ë‹µ ë ˆì´ë¸”
accuracy(x, t) : ì •í™•ë„ë¥¼ êµ¬í•œë‹¤.
numerical_gradient(x, t) : ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ìˆ˜ì¹˜ ë¯¸ë¶„ìœ¼ë¡œ êµ¬í•¨(ì• ì¥ê³¼ ê°™ìŒ)
gradient(x, t) : ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ì˜¤ì°¨ì—­ì „íŒŒë²•ìœ¼ë¡œ êµ¬í•¨
"""


"""
ğŸš€ ì´ ì½”ë“œê°€ ì¤‘ìš”í•œ ì´ìœ 

ì´ ì½”ë“œëŠ” ì‹ ê²½ë§ í•™ìŠµì˜ ì „ì²´ ê³¼ì •ì„ ë‹´ê³  ìˆì–´!
íŠ¹íˆ, ì˜¤ì°¨ì—­ì „íŒŒë²•ì„ í™œìš©í•˜ì—¬ ê¸°ìš¸ê¸°(gradient)ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•œë‹¤ëŠ” ì ì´ í•µì‹¬ì´ì•¼.
ê¸°ìš¸ê¸°ë¥¼ ì •í™•í•˜ê²Œ ê³„ì‚°í•´ì•¼ ì†ì‹¤ì„ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ì•¼.

ğŸ¯ ë°˜ë“œì‹œ ì´í•´í•´ì•¼ í•˜ëŠ” í•µì‹¬ ë¶€ë¶„

1ï¸âƒ£ ìˆœì „íŒŒ(Forward Propagation) ê³¼ì •
â†’ predict(x), loss(x, t)
â†’ ì…ë ¥ ë°ì´í„°ê°€ ì‹ ê²½ë§ì„ í†µê³¼í•˜ë©´ì„œ ì˜ˆì¸¡ê°’ì„ ìƒì„±í•˜ëŠ” ê³¼ì •

2ï¸âƒ£ ì†ì‹¤ í•¨ìˆ˜(Loss Function)
â†’ loss(x, t)ì—ì„œ SoftmaxWithLossë¥¼ í†µí•´ ì†ì‹¤ì„ ê³„ì‚°
â†’ ì†ì‹¤ì€ ì‹ ê²½ë§ì´ ì–¼ë§ˆë‚˜ í‹€ë ¸ëŠ”ì§€ ì¸¡ì •í•˜ëŠ” ì—­í• ì„ í•¨

3ï¸âƒ£ ê¸°ìš¸ê¸° ê³„ì‚°(Gradient Calculation)

numerical_gradient(x, t) â†’ ìˆ˜ì¹˜ ë¯¸ë¶„ìœ¼ë¡œ ê¸°ìš¸ê¸° ê³„ì‚° (ë¹„íš¨ìœ¨ì )
gradient(x, t) â†’ ì˜¤ì°¨ì—­ì „íŒŒë²•ìœ¼ë¡œ ê¸°ìš¸ê¸° ê³„ì‚° (íš¨ìœ¨ì )
â†’ ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•´ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •
4ï¸âƒ£ ì—­ì „íŒŒ(Backpropagation) ê³¼ì •

gradient(x, t) í•¨ìˆ˜ì—ì„œ ì—­ì „íŒŒë¥¼ ìˆ˜í–‰
dout = self.lastLayer.backward(dout) â†’ ë§ˆì§€ë§‰ Softmax ì¸µì—ì„œ ì‹œì‘
for layer in layers: â†’ ê±°ê¾¸ë¡œ ìˆœíšŒí•˜ë©´ì„œ backward(dout) ìˆ˜í–‰
self.layers['Affine1'].dW â†’ ê° ì¸µì˜ ê°€ì¤‘ì¹˜(W), í¸í–¥(b)ì˜ ë³€í™”ëŸ‰(âˆ‡)ì„ ì €ì¥
ì´ ê³¼ì •ì´ í•µì‹¬! ì‹ ê²½ë§ì´ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ì—¬ ë” ë‚˜ì€ ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŒ
ğŸ§  ì´í•´ë¥¼ ë•ëŠ” ì˜ˆì œ

ë„¤íŠ¸ì›Œí¬ê°€ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ í•™ìŠµí•˜ëŠ” ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ì•„.

1ï¸âƒ£ ìˆœì „íŒŒ

x
x â†’ Affine1(ê°€ì¤‘ì¹˜ W1 ì ìš©) â†’ ReLU1(ë¹„ì„ í˜• ë³€í™˜) â†’ Affine2(W2 ì ìš©) â†’ SoftmaxWithLoss â†’ ì†ì‹¤ ê³„ì‚°
2ï¸âƒ£ ì—­ì „íŒŒ

ì†ì‹¤ì„ ì¤„ì´ê¸° ìœ„í•´ ì—­ì „íŒŒ(backpropagation) ìˆ˜í–‰
ì†ì‹¤ì´ Affine2, Relu1, Affine1ì„ ê±°ê¾¸ë¡œ ì§€ë‚˜ê°€ë©´ì„œ ê° ì¸µì˜ ê°€ì¤‘ì¹˜(W, b)ì˜ ë³€í™”ëŸ‰(ê¸°ìš¸ê¸°)ì„ ê³„ì‚°
ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ ë” ë‚˜ì€ ì˜ˆì¸¡ì„ ë§Œë“¤ë„ë¡ í•¨
âœ… ê²°ë¡ : ì´ ì½”ë“œì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„

ì˜¤ì°¨ì—­ì „íŒŒë²•(Backpropagation)ì„ ì´í•´í•˜ë©´ ì‹ ê²½ë§ì´ í•™ìŠµí•˜ëŠ” ì›ë¦¬ë¥¼ ì•Œ ìˆ˜ ìˆìŒ!
gradient(x, t) í•¨ìˆ˜ì—ì„œ ê°€ì¤‘ì¹˜ W, í¸í–¥ bì˜ ê¸°ìš¸ê¸°ë¥¼ ì—­ì „íŒŒë¥¼ í†µí•´ êµ¬í•˜ëŠ” ê³¼ì •ì´ í•µì‹¬
ì´ ê¸°ìš¸ê¸°ë¥¼ ì´ìš©í•´ ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ë©´ ì‹ ê²½ë§ì´ ì ì  ë” ë˜‘ë˜‘í•´ì§ ğŸš€

"""


class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size,
        weight_init_std=0.01):
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.params = {}
        self.params['W1'] = weight_init_std * \
            np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * \
            np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)

        # ê³„ì¸µ ìƒì„±
        self.layers = OrderedDict()
        self.layers['Affine1'] = \
            Affine(self.params['W1'], self.params['b1'])
        self.layers['Relu1'] = Relu()
        self.layers['Affine2'] = \
            Affine(self.params['W2'], self.params['b2'])
        self.lastLayer = SoftmaxWithLoss()

    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)

        return x

    # x : ì…ë ¥ ë°ì´í„°, t : ì •ë‹µ ë ˆì´ë¸”
    def loss(self, x, t):
        y = self.predict(x)
        return self.lastLayer.forward(y, t)

    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        if t.ndim != 1:
            t = np.argmax(t, axis=1)

        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy

    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)

        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])

        return grads

    def gradient(self, x, t):
        # ìˆœì „íŒŒ
        self.loss(x, t)

        # ì—­ì „íŒŒ
        dout = 1
        dout = self.lastLayer.backward(dout)

        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)

        # ê²°ê³¼ ì €ì¥
        grads = {}
        grads['W1'] = self.layers['Affine1'].dW
        grads['b1'] = self.layers['Affine1'].db
        grads['W2'] = self.layers['Affine2'].dW
        grads['b2'] = self.layers['Affine2'].db

        return grads